\section{Decisiones de diseño} \label{decisiones}
En las siguientes subsecciones se van a detallar las decisiones de diseño que se han tenido en cuanta a la hora de desarrollar e implementar este sistema. Algunas de estas decisiones de diseño son las mismas que se tomaron cuando se desarrolló el módulo de indexación de documentos, aunque se han realizado algunos cambios. 

\subsection{Lenguaje de programación y sistema operativo elegido}

Para implementar el sistema, se ha decidido utilizar el lenguaje de
programación Python (\cite{Python}). Se ha seleccionado este lenguaje
de programación ya que permite el uso de estructuras como diccionarios
(tablas hash), listas, manejadores de archivos, llamadas al sistema,
definición de patrones mediante expresiones regulares, etc. Dichos
elementos han facilitado el desarrollo del sistema, ya
que Python los gestiona de una manera eficaz y permite su uso de uso
de una manera sencilla. 

Por otra parte, se ha elegido un sistema UNIX porque resulta más
cómodo a la hora de llevar a cabo la implementación. 

\subsection{Diseño del sistema}

\subsubsection{Diseño multicapa}
El sistema se ha desarrollado siguiendo el enfoque multicapa, para
desacoplar las operaciones del dominio de las operaciones de
persistencia y presentación, consiguiendo así un sistema extensible y
reutilizable. 

Uma muestra de reutilización es que las clases de la capa de presentación conocen a las
clases de dominio, pero no al revés, por
lo que podría cambiarse la interfaz de usuario sin tener que cambiar
las clases de dominio. 

Por otra parte, el dominio es totalmente independiente de la persistencia, pues se implementan el patrón DAO (también llamado Fabricación Pura), encargados de gestionar la persistencia, tanto en la base de datos como en un fichero. Además, el DAO que se usa para almacenar la información en la base de datos, utiliza a su vez un patrón Agente Singleton, encargado de inicializar la conexión de la base de datos si no existe ya una
instancia del agente, cerrar la conexión y ejecutar las sentencias SQL que recibe del patrón DAO. 

De este modo, el patrón DAO se ha utilizado para
desacoplar el dominio de la persistencia, evitando así que las propias
clases del dominio creen las secuencias SQL necesarias para gestionar
la base de datos, pasando ésto a ser responsabilidad del patrón
DAO. Por tanto, si en algún momento se cambia el sistema gestor de
base de datos, las clases de dominio no se verían afectadas, ya que
sólo habría que cambiar el agente y el DAO.

Esta división en capas y las diferentes clases diseñadas e implementadas se detallan en el apartado siguiente.

\subsubsection{Diagrama de clases}
En la Figura \ref{fig:clases} se muestran las clases que conforman el sistema. Como puede observarse en la figura, y como se mencionó en el apartado anterior, se han diseñado tres capas (representadas por paquetes) que permiten desacoplar unas clases de otras, ya que sólo se utilizan dependencias de uso entre clases.

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.3]{./images/SearchEngine}
	\caption{Diagrama de clases del sistema desarrollado}
	\label{fig:clases}
\end{figure} 

A continuación, se explica brevemente la responsabilidad de cada una de las clases y el funcionamiento de sus métodos más importantes.

\begin{milista}
	\item a
\end{milista}

\subsection{Diseño de la base de datos}

Como sistema gestor de base de datos se ha optado por utilizar MySQL,
ya que es ligero, libre, gratuito (ya que no se destina a fines
comerciales) y compatible con otros sistemas, como Windows. 

Por otra parte, como ya se comentó en la documentación del módulo de indexación, el diseño de la base de datos era el que se muestra en el diagrama de la Figura \ref{fig:BD}. 

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.75]{./images/BD}
	\caption{Diagrama de la base de datos antigua}
	\label{fig:BD}
\end{figure} 

Sin embargo, ha sido necesario modificar ligeramente este diseño, para optimizar las búsquedas y consumir menos tiempo. El cambio que se ha introducido es simplemente un cambio de la clave primaria de la tabla \textit{posting$\_$file}, pasando a tener una clave autonumérica. Con esto (y optimizando las sentencias SQL, como se detallará posteriormente), se ha reducido tanto el tiempo de búsqueda, como el de indexación.

Así, el nuevo diagrama de tablas se muestra en la Figura \ref{fig:newDB}, mientras que las tablas que existen en el sistema son: 

\begin{milista}
	\item \textbf{dic}: implementa el diccionario de
          términos. Tiene los campos \textit{term} y
          \textit{num$\_$docs}, que representan cada uno de los
          términos encontrados, junto con el número de documentos
          donde aparece cada término. La clave primaria es el término.
	\item \textbf{doc}: implementa la tabla de documentos. Tiene
          los campos \textit{id$\_$doc}, \textit{title} y
          \textit{path} para almacenar un identificador único de
          documento, el título del documento y la ruta del sistema
          donde se almacena una copia de dicho documento.  
	\item \textbf{posting$\_$file}: implementa el
          posting$\_$file. Contiene los campos \textit{id}, \textit{term},
          \textit{id$\_$doc} y \textit{frequency} para representar la
          frecuencia con la que aparece un término en un
          documento. Por tanto, el campo \textit{term} hace referencia
          a términos del diccionario, y el campo \textit{id$\_$doc}
          hace referencia a los documentos de la tabla de documentos. 
\end{milista}

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.75]{./images/newBD}
	\caption{Diagrama de la nueva base de datos}
	\label{fig:newBD}
\end{figure} 

\paragraph{Optimización de las sentencias SQL en el indexador de documentos} En la primera versión que se desarrolló del módulo encargado de indexar los documentos, los términos que se iban encontrando en un documento, se iban insertando uno a uno en la tabla \textit{posting$\_$file} y en el diccionario, además de ir actualizando la frecuencia de aparición del diccionario, con una sentencia \textbf{update}. Esto hacía que el acceso a la base de datos ocupase gran parte del tiempo, ya que se realizaban numerosas sentencias \textbf{insert} y \textbf{update}, para actualizar la tabla \textit{posting$\_$file} y la tabla \textit{dic} con los términos que se iban parseando.

Estudiando la sintaxis completa de la sentencia \textbf{insert}, se decidió hacer una inserción de múltiples filas en la tabla \textit{posting$\_$file}, donde cada fila corresponde a cada uno de los términos encontrados en un documento (junto con el identificador del documento y su frecuencia). Así, la sentencia \textbf{insert} que se utiliza es similar a la siguiente, que aparece en el manual de MySQL:

\makebox{\textbf{INSERT INTO tbl$\_$name [(col$\_$name,...)] VALUES (expr),(expr), ...}}

Del mismo modo, la sentencia \textbf{insert} permite hacer cambios en una columna cuando se intenta insertar una fila con clave primaria duplicada. Por tanto, esto puede utilizarse para realizar inserciones múltiples y, a la vez, actualizaciones en el diccionario, ya que si el término no se encontraba en el diccionario, se insertará, y si ya se encontraba, ocurrirá un fallo de clave duplicada (porque la clave primaria de la tabla \textit{dic} es el término) y se actualizará con la frecuencia correspondiente. La sentencia utilizada para insertar y actualizar términos en el diccionario es el siguiente:

\makebox{\textbf{INSERT INTO tbl$\_$name [(col$\_$name,...)] VALUES (expr),(expr), ...}} \\ 
\indent \makebox{\textbf{ON DUPLICATE KEY UPDATE num$\_$docs=num$\_$docs+VALUES(num$\_$docs)}}

Con estas cambios, el tiempo de indexación se ha reducido unos 22 minutos con respecto al módulo de indexación de la primera versión. En la sección \ref{estadisticas} se muestran algunas estadísticas de uso, tanto del módulo antiguo como del nuevo sistema.

\paragraph{Optimización de las sentencias SQL en la búsqueda de documentos} En un primer momento, para obtener todos los datos (frecuencias, identificador del documento, titulo, etc.) de los documentos que tenían términos en común con la pregunta, se pensó en hacer varias sentencias \textbf{select} por separado, pero esto consumía demasiado tiempo para una búsqueda.

Por tanto, se optó por hacer una única consulta componiendo las tres tablas y obteniendo así todos los datos necesarios para calcular los pesos y la relevancia. Esta consulta es la que se muestra a continuación: 

\makebox{\textbf{SELECT posting$\_$file.term,posting$\_$file.id$\_$doc,frequency,num$\_$docs,title}} \\
\indent \makebox{\textbf{FROM posting$\_$file JOIN dic JOIN doc}} \\
\indent \makebox{\textbf{ON dic.term=posting$\_$file.term AND posting$\_$file.id$\_$doc IN}} \\
\indent \makebox{\textbf{(SELECT DISTINCT p1.id$\_$doc FROM posting$\_$file p1 WHERE}} \\
\indent \makebox{\textbf{p1.term IN (\textit{lista de términos de la pregunta}))}}

Con esa sentencia, la consulta tardaba más de 40 segundos (para el término ''linux'') en procesarse, por lo que era necesario realizar unos cambios. Estudiando el manual de la sentencia \textbf{select} de MySQL, se observó que se puede elegir el índice (clave ajena) con el cuál combinar las tablas, lo que puede acelerar la búsqueda. Además, se puede indicar que esa sentencia tenga alta prioridad, ejecutando la sentencia en la base de datos antes que cualquier otra. Por otro lado, se optó también por crear una vista temporal con los resultados de la subconsulta anterior, para luego combinar con esa vista, en vez de tener una consulta con una subconsulta. Así, obtenemos estas sentencias:

\makebox{\textbf{CREATE VIEW view$\_$name AS SELECT DISTINCT p1.id$\_$doc}}\\
\indent \makebox{\textbf{FROM posting$\_$file p1 USE INDEX (term) WHERE}} \\
\indent \makebox{\textbf{p1.term IN (\textit{lista de términos de la pregunta}))}}

\makebox{\textbf{SELECT HIGH$\_$PRIORITY posting$\_$file.term, posting$\_$file.id$\_$doc, frequency,}}\\
\indent \makebox{\textbf{num$\_$docs, title}} \\
\indent \makebox{\textbf{FROM posting$\_$file USE INDEX (term,id$\_$doc) JOIN view$\_$name v JOIN dic}}\\
\indent \makebox{\textbf{JOIN doc }} \\
\indent \makebox{\textbf{ON posting$\_$file.id$\_$doc=v.id$\_$doc AND dic.term=posting$\_$file.term}}\\
\indent \makebox{\textbf{AND posting$\_$file.id$\_$doc=doc.id$\_$doc}}

\subsection{Desarrollo del sistema}
Para terminar, en esta subsección se comentan las diferentes
decisiones de diseño, así como los problemas que han surgido a la hora
de desarrollar el módulo de importación e indexación de documentos. 

  

\subsubsection{Analizador de documentos}

\paragraph{Conexión con la base de datos} En una primera iteración del
desarrollo de la aplicación, la conexión con la base de datos se
creaba cada vez que se quería acceder a ella, como, por ejemplo, al
actualizar la frecuencia de un término en el \textit{posting$\_$file},
cerrándose al terminar el acceso. Esto consumía mucho tiempo y el
acceso a disco era muy elevado, por lo que se ha optado por
inicializar la conexión con la base de datos al lanzar la aplicación, y
cerrarla cuando la aplicación finaliza. 

\paragraph{Copia de documentos} Cuando se analiza un documento para
realizar su indexación, en un primer momento se copiaba el documento
en la base de datos documental del sistema leyendo línea por línea. Este proceso
era algo lento, por lo que finalmente se ha utilizado una llamada al
sistema implementada en C, consiguiendo que la copia del documento
completo sea instantánea.  

\paragraph{Parser} Para realizar el tratamiento de los documentos y
prepararlos para su indexación, se ha implementado el método
\textit{parser}. Dicho módulo recibe una línea del documento a indexar
y realiza la siguientes acciones:  
\begin{milista}
	\item En primer lugar, se escapan los caracteres
          ''$\backslash$'' y '' ' '' de las palabras que los
          contengan, ya que si se intenta insertar un término con esos
          caracteres, la sentencia sql no se forma de manera correcta
          y se provoca una excepción al ejecutar esa sentencia en la
          base de datos.  
	\item Se definen separadores de palabras, que son tanto los
          signos de puntuación, como los espacios en blanco
          (incluyendo saltos de línea, tabuladores, etc). Se define
          también un patrón para detectar direcciones IP. 
	\item Se sustituyen las vocales acentuadas de las palabras por
          vocales sin acentuar. 
	\item En cada una de las palabras de la línea, se sustituyen
          los separadores que pueda contener la palabra por un espacio
          en blanco y se divide la palabra por dichos espacios, obteniendo así una lista de palabras, que recibe
          el siguiente tratamiento: 
	\begin{enumerate}
	\item Se eliminan los espacios en blanco.
	\item Si alguna palabra de la lista que se ha obtenido al
          dividir una palabra por separadores se encuentra en la
          stop$\_$list, dicha palabra no se divide, ya que quedarían
          palabras sin sentido. Por ejemplo, si la palabra ''F-14'' se
          divide en ''[F,'' '',14]'', al encontrarse ''F'' en la
          stop$\_$list, se almacenaría todo el término sin separarse. 
	\item Si al dividir la palabra solo se obtiene una palabra de
          la stop$\_$list rodeada de espacios, dicha palabra se
          ignora. Por ejemplo, al separar la palabra ''$\backslash$t
          already?'', se obtendria la lista ['' '', '' '', already,''
          ''], que sólo contiene una palabra de la stop$\_$list
          rodeada de espacios, lo cual no tiene sentido.  
	\item En las direcciones web o e-mail, el tratamiento es el
          mismo: se divide la palabra por separadores, obteniendo una
          lista y almacenando las palabras por separado. \\
          \indent Esto soluciona el siguiente problema: si el texto no tiene una
          escritura correcta y apareciese la palabra
          ''juanro.1987@gmail.com.Hola'', con este tratamiento se
          almacenarían las palabras ''juanro'', ''1987'', ''gmail'',
          ''com'' y ''hola''. Sin embargo, si tratasemos un e-mail
          como una palabra completa hasta que se encontrase un
          espacio, se almacenaría como término
          ''juanro.1987@gmail.com.Hola'', que luego no se encontraría en el diccionario 
          si alguien consulta por la dirección de e-mail
          ''juanro.1987@gmail.com''. En nuestro caso, como este mismo
          parser se va a aplicar a las consultas, la cadena anterior
          recibiría el mismo tratamiento y podría recuperar los
          términos que se habían almacenado en el diccionario por separado. 
	\end{enumerate}
\end{milista}

En resumen, este módulo parser se ha implementado para dar el mismo tratamiento tanto a las cadenas de los documentos a indexar como a las cadenas de consultas.

\paragraph{Codificación de documentos y de la base de datos} Tras
realizar múltiples pruebas con diferentes documentos, se observaron
fallos a la hora de recuperar y almacenar términos en la base de
datos. Estos fallos eran debidos a la codificación de los documentos,
que debe ser UTF-8 para el corrrecto funcionamiento en sistemas
UNIX. Por tanto, todos los documentos de la colección se han
estandarizado a la codificación UTF-8, al igual que las diferentes
tablas de la base de datos. 

\paragraph{Posting$\_$file} Se ha optado por almacenar el
posting$\_$file de cada documento en memoria en forma de diccionario
(tabla hash), para conseguir que los accesos sean más rápidos. Se han
realizado pruebas y para un documento de unas 25.000 líneas, el tamaño
del posting$\_$file no excede de los 13MB en memoria. Por tanto, esta
opción es más eficiente en cuanto a tiempo, ya que sólo se vuelca el diccionario a la base de datos cuando se termina de procesar el documento, en lugar de ir accediendo a la base de datos por cada uno de los términos que se deben almacenar.

\paragraph{Memoria caché} Se ha implementado una memoria caché, pero,
tal como se puede analizar en la sección de estadísticas
(\ref{estadisticas}), no se han obtenido los resultados esperados
(minimizar el número de accesos a disco). La idea se ha implementado
con dos diccionarios:
\begin{milista}
\item \texttt{new}. Contiene nuevos términos que no están en la base
  de datos y que serán insertados con la sentencia ``INSERT''.
\item \texttt{old}. Contiene términos que ya estaban en la base de
  datos, es decir, que han sido recuperados de ésta, y serán
  actualizados con la sentencia ``UPDATE''.
\end{milista}
Dado que no hemos llegado al suficiente grado de granuralidad en este
diseño, aunque no minimizamos el número de accesos a disco, al menos
hacemos que éstos sean en intervalos más espaciados en el tiempo, de
modo que no se ocupa el recurso de forma constante.

\paragraph{Psyco} Psyco es un módulo externo que realiza una
compilación \emph{just-in-time} y que incrementa la
velocidad de ejecución del código Python (ya que éste no se
interpretará). Gracias a este módulo el tiempo de ejecución de la
aplicación se puede reducir hasta la mitad.






