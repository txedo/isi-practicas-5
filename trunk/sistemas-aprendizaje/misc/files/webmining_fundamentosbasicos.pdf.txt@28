Web Mining: Fundamentos Basicos
Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca
fdegyves@usal.es

Resumen. La web es uno de las aplicaciones o fenomenos mas importantes que han surgido en los ultimos tiempos, ¿por que?, no hay mas que ver la rapida aceptacion que tuvo en la sociedad, ya que aqui descubrieron un medio significativo para exponer riquezas de informacion. Los motores de busqueda son actualmente los mejores repositorios de informacion de la web, esto es debido al volumen de datos que contienen. Los usuarios acuden a este medio con el fin de localizar informacion mas sin embargo si no se utilizan adecuadamente o no se busca bien, pueden ser no fructiferos. La web mining juega un rol importante hacia lograr la efectividad en las relaciones de patrones interesantes.

1. Introduccion
El crecimiento desmedido de la informacion que se encuentra en la web ha sido exponencial debido a la necesidad de los usuarios (personas fisicas, empresas, universidades, gobierno, etc.) de contar con datos para la interrelacion en el mundo globalizado. De acuerdo con Baeza­Yates, la informacion de la web es finita pero el numero de paginas web es infinita 0 [ 1]. Actualmente existen alrededor de 4 mil millones de paginas estaticas, es decir la informacion que poseen los buscadores web, mas sin embargo es importante mencionar que la mayoria de las paginas web y que no son indexables que existen en la web son dinamicas, es decir son aquellas que se generan automaticamente con datos extraidos de bases de datos [ 2]. Existen diferentes problemas a los que se enfrentan los usuarios debido al crecimiento exponencial. Uno de esos problemas es el que representa encontrar informacion relevante, esto es por dos aspectos muy relevantes, la baja precision y la escasa cobertura. La escasa cobertura es debido a que no todos los motores de busqueda tienen la suficiente capacidad de indexar la web, debido a varios factores; el ancho de banda, el espacio de disco duro, el costo economico, etc. La web mining actualmente es un area de investigacion extensa dentro de varios grupos de investigadores, especialmente interesados debido al alto crecimiento de la informacion que existe en la web y por el movimiento economico que ha generado el e-commerce y sobre todo para intentar resolver los problemas que se han mencionado anteriormente, ya sea de manera directa o indirectamente. Actualmente

2 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

lo que se ha pretendido realizar es aprender de acuerdo a los comportamientos de los usuarios en su andar por la web y asi proporcionarles informacion realmente relevante, util y personalizada en muchos casos. El presente documento pretende adentrarnos un poco en el mundo de la web mining, permitiendonos conocer los aspectos basicos tales como, el proceso general de la web mining asi como unas breves aproximaciones del web content mining, web structure mining y de la web usage mining

2. Particularidades de la Web
Es comun encontrarse en la web con ciertas caracteristicas preponderantes que parecieran problemas, sin embargo podrian catalogarse como oportunidades sin precedentes para la obtencion de informacion y mejoramiento de la web. En los tiempos actuales, es comun encontrar casi todo tipo de informacion en la web en cantidades descomunales y facilmente accesibles. La informacion en la Web por lo regular es heterogenea, es decir muchas paginas presentan la misma o similar informacion usando formatos diferentes. Podemos decir que la informacion es redundante. La Web normalmente esta compuesta por una mezcla de tipos de informacion, por ejemplo, contenido principal, anuncios, paneles de navegacion, noticias de copyright, etc. Para una aplicacion en particular solo parte de la informacion es util y el resto es basura o no util [ 3]. La informacion de la web cambia constantemente, es decir es dinamica.

3. Web mining y su proceso
Algunos autores definen a la web mining como el uso de tecnicas para descubrir y extraer de forma automatica informacion de los documentos y servicios de la web 0]. Segun M. Scotto [ 4] la web mining es el proceso de descubrir y analizar informacion "util" de los documentos de la Web. Sin embargo y tomando en cuenta lo expuesto en la introduccion la mineria web se puede definir como el descubrimiento y analisis de informacion relevante que involucra el uso de tecnicas y acercamientos basados en la mineria de datos (Data Mining) orientados al descubrimiento y extraccion automatica de informacion de documentos y servicios de la Web, teniendo en consideracion el comportamiento y preferencias del usuario. En la web mining, los datos pueden ser coleccionados en diferente niveles; en el area del servidor, en el lado del cliente (cookies), en los servidores proxys (log files), etc.

Web Mining: Fundamentos Basicos

3

De acuerdo con Etzioni 0] el proceso general de la web mining es el siguiente: Recuperacion de Informacion (IR) No referimos basicamente al proceso del descubrimiento automatico de documentos relevantes de acuerdo a una cierta busqueda. Documentos relevantes disponibles en la web tales como noticias electronicas, newsgroups, newswires, contenido de las html, etc. Extraccion de Informacion(IE) Tiene como objetivo transformar los documentos extraidos en el proceso de recuperacion de informacion, en documentos que sean mas digeribles, faciles de leer y de analizar. Generalization Reconocimiento de Patrones generales de una pagina en particular o bien tambien patrones de diferentes paginas. Analysis Una vez que los patrones han sido identificados, la parte humana juega un papel importante haciendo uso de herramientas adecuadas para entender, visualizar e interpretar los patrones

3.1 .Motores de busqueda Los motores de busqueda han tenido un exito considerado gracias a la enorme necesidad que tienen los usuarios de indagar informacion no disponibles en sitios fisicos y si en la Internet. Un motor de busqueda tiene como objetivo indexar archivos almacenados en los servidores web, un ejemplo son los buscadores de Internet. El resultado de la busqueda es un listado de direcciones Web en los que se mencionan temas relacionados con las palabras clave buscadas [ 6]. Basandonos en las descripciones hechas por Henzinger [ 7] podemos decir que los motores de busqueda estan integrados basicamente por tres grandes componentes; el crawler, el indexador y el query processor. Crawler: Recorren las paginas recopilando informacion sobre los contenidos de las paginas. Recolectan paginas de manera recursiva a partir de un conjunto de links de paginas iniciales. Cuando buscamos una informacion en los motores, ellos consultan

4 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

su base de datos, y nos la presentan clasificados por su relevancia. De las webs, los buscadores pueden almacenar desde la pagina de entrada, a todas las paginas de la web. Indexador: el objetivo principal del indexador es procesar las paginas coleccionadas por el crawler. La indexacion proporciona agilidad en las busquedas, lo que se traduce en mayor rapidez a la hora de mostrar resultados. Query processor: procesa las consultas de los usuarios y regresa resultados de acuerdo a esas consultas y de acuerdo a un algoritmo de posicionamiento.

3.2 Taxonomia de la Web mining

3.2.1 Mineria de contenido de la web Su objetivo es la recogida de datos e identificacion de patrones relativos a los contenidos de la web y a las busquedas que se realizan sobre los mismos. Es decir son los datos reales que se entregan a los usuarios, los datos que almacenan los sitios web[ 8]. La mineria de contenidos consiste de datos desestructurados tales como texto libre, semi-estructurado como documentos HTML, y mas estructurados como datos en tablas o paginas generadas con datos de BD. Existen dos grupos de estrategias sobre mineria de contenidos: aquellas que minan directamente el contenido de los documentos y aquellas que mejoran en la busqueda de contenidos. 3.2.2 Mineria de estructura de la web La mineria de estructura intenta descubrir el modelo subyacente de las estructuras de los enlaces del web. El modelo se basa en la topologia de los hiperenlaces con o sin la descripcion de los enlaces. Este modelo puede ser usado para categorizar las paginas web y es util para generar informacion tal como la similitud y relacion entre diferentes paginas web [ 9]. Es decir pretende revelar la estructura real de un sitio web a traves de la recogida de datos referentes a su estructura y, principalmente a su conectividad. Tipicamente tiene en cuenta dos tipos de enlaces: estaticos y dinamicos. 3.2.3 Mineria de uso de la web La mineria de uso intenta dar sentido a los datos y comportamientos generados en las sesiones de navegacion del web. Es decir son aquellos datos que describen el uso al

Web Mining: Fundamentos Basicos

5

cual se ve sometido un sitio, registrado en los logs de acceso a de los servidores web. A partir de esta informacion se podria concluir, por ejemplo, que documento visitado no tiene razon de ser, o si una pagina no se encuentra en los primeros niveles de jerarquia de un sitio [ 8]. Analizar los logs de diferentes servidores web, puede ayudar a entender el compartamiento del usuario, la estructura de la web, permitiendo de este modo mejorar el diseno de esta coleccion de recursos [ 10].
WEB MINING

WEB CONTENT MINING

WEB STRUCTURE MIINING

WEB USAGE MINING

4. Web Content Mining
La mineria de contenido, tiene como principal objetivo otorgar datos reales o finales a los usuarios que interactuan con la Web. Es decir, extraer informacion "util" de los contenidos de las paginas web. Generalmente la informacion disponible, se encuentra de forma no estructurada (mineria de Texto), semi-estructurada y un poco mas estructurada como es el caso de tablas html generadas automaticamente con informacion de bases de datos. De acuerdo con Raymon Kosala y Hendrick Blockeel 0 [ 9], la mineria de contenido puede ser diferenciada desde dos puntos de vista; desde el punto de vista de la Recuperacion de Informacion (IR) y desde la vista de Base de Datos (DB). Es decir asistir en el proceso de recogida de informacion o mejorar la informacion encontrada por los usuarios, usualmente basada en las solicitudes hechas por ellos mismos (IR). Desde el punto de vista de DB principalmente trata de modelar los datos e integrarlos en la Web a traves de queries sofisticadas

4.1. Mineria de Contenido desde el punto de vista de Recuperacion de Informacion y Extraccion de Informacion. La recuperacion de informacion es el proceso de encontrar el numero apropiado de documentos relevantes de acuerdo a una busqueda hecha en una coleccion de documentos. La IR y la web mining tienen diferentes objetivos, es decir la web mining no busca remplazar este proceso. La web mining pretende ser utilizada para

6 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

incrementar la precision en la recuperacion de informacion y mejorar la organizacion de los resultados extraidos 0 [ 5]. La recuperacion de informacion es altamente popular en grandes empresas del mundo web, las cuales hacen uso de este tipo de sistemas, la maquinas de busqueda (google y altavista), directorios jerarquicos (yahoo) y otros tipos de agentes y de sistemas de filtrado colaborativos. La diferencia principal, independientemente de las tecnicas que usan, que existe entre la Recuperacion de la informacion y la Extraccion de la Informacion recae principalmente en que uno recupera documentos relevantes de una coleccion y la otra recupera informacion relevante de dichos documentos. La IE se centra principalmente en la estructura o la representacion de un documento mientras que la IR mira al texto en un documento como una bolsa de palabras en desorden [ 11]. Podemos decir que dichas tecnicas son complementarias una de otra y usadas en combinacion pueden generar valor agregado. Datos no estructurados, semi-estructurados y estructurados, son los objetivos de la Extraccion de Informacion, generalmente para los datos no estructurados se hacen uso de tecnicas de Lenguaje Natural. Dichas reglas son generalmente basadas en el uso de relaciones sintacticas entre palabras y clases semanticas. Reconocimiento de objetos de dominios tales como, nombres de personas y companias, analisis sintactico y etiquetado semantico, son algunos de los pasos para la extraccion de informacion en documentos no estructurados. Recientemente se ha hecho uso de una tecnologia llamada Text mining , que hace referencia principalmente al proceso de extraccion de informacion y conocimiento interesante, no trivial desde documentos no estructurados [ 6]. Las principales categorias de la Web Text mining son Text Categorization, Text Clustering, association analysis, trend prediction. Text Categorization: dada una predeterminada taxonomia, cada documento de una categoria es clasificada dentro de una clase adecuada o mas de una. Es mas conveniente o facil realizar busquedas especificando clases que buscando en documentos. Actualmente existen varios algoritmos de text categorization, dentro de los cuales encontramos, K-nearest, neighbor-algorithm y naive bayes algorithm. Text Clustering: el objetivo de esta categoria es el de dividir una coleccion de documentos en un conjunto de clusteres tal que la similitud intra-cluster es minimizada y la similitud extra-cluster es maximizada. Podemos hacer uso de text clustering a los documentos que fueron extraidos por medio de una maquina de busqueda. Las busquedas de los usuarios referencian directamente a los clusters que son relevantes para su busqueda. Existen dos tipos de text clustering, clustering jerarquico y clustering particional (G-HAC y k-means).[ 13]

Web Mining: Fundamentos Basicos

7

4.2. Mineria de Contenido desde el punto de vista de BD La Web es una fuente enorme de documentos en linea que regularmente contienen datos semi-estructurados. La Extraccion de informacion en la web se afronta de diferente manera a lo antes hecho, ahora hay que enfrentarse a un volumen extenso de documentos web, a los documentos nuevos que aparecen con periocidad y al cambio en el contenido de los documentos web. Una gran parte de los documentos o paginas web contienen datos semi-estructurados y estructurados y generalmente o siempre contienen informacion a traves de links[ 14]. El objetivo principal que tiene la web content mining desde el punto de vista de BD es que busca representar los datos a traves de grafos etiquetados. La publicacion de datos semi-estructurados y estructurados en la web ha crecido fuertemente en los ultimos anos y existe la tendencia a seguir creciendo, mas sin embargo el crecimiento ha sido preponderante en las "hidden Web" [ 13 14] paginas ocultas, las cuales son generadas automaticamente con datos de bases de datos a traves de consultas hechas por usuarios. Dichas paginas no son accesibles para los crawlers y para las maquinas de busqueda no estan a su alcance. Es asi pues que existe la necesidad de crear ciertas aplicaciones o herramientas para la extraccion de informacion de tales paginas. Para la obtencion de dicha informacion en las web se hacen uso actualmente de los llamados "wrappers". Los wrappers pueden ser vistos como procedimientos para extraccion de contenido de una fuente particular de informacion La extraccion de estos datos permite otorgar valor agregado a los servicios, por ejemplo, en los comparativos de compras, meta busquedas, etc. Existen varios enfoques para la extraccion de informacion estructurada; manual wrapper, wrapper induction y el enfoque automatico [ 3]. El primero consiste en escribir un programa para extraccion de informacion de acuerdo con los patrones observados en un Web site en especifico. Los segundos consisten en identificar un grupo de paginas de entrenamiento y un sistema de aprendizaje generara reglas a partir de ellas, finalmente dichas reglas seran aplicadas para obtener objetos identificados dentro de paginas Web. Finalmente el metodo automatico tiene como objetivo principal identificar patrones de las paginas web y luego usarlas para extraer informacion. Seguramente este ultimo es el metodo mas utilizado en la actualidad para extraer informacion de la Web.

5. Web Structure Mining
De acuerdo con WangBin [¡Error! No se encuentra el origen de la referencia.] las estructuras de links permiten otorgar mayor informacion que otro documento normal. La Web Structure Mining se centra principalmente en la estructura de los hiperlinks de la web, es decir interesada en la entrada y salida de links de las paginas. Los links que

8 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

apuntan a una pagina puede sugerir la popularidad de la misma, mientras que los links que salen de la pagina demuestran los topicos o la riqueza de contenido. Algoritmos como el PageRank y los HITS son usados con frecuencia para modelar la topologia de la web. En PageRank, cada pagina Web tiene una medida de prestigio que es independiente de cualquier necesidad de informacion o pregunta. En linea general, el prestigio de una pagina es proporcional a la suma de las paginas que se ligan a el. PageRank es un valor numerico que representa lo importante que es una pagina en la web. Para Google, cuando una pagina(A) enlaza a otra(B), es como si la pagina(A) que tiene el enlace, votara a la pagina enlazada(B). Mientras mas votos tenga una pagina, mas importante sera la pagina. Tambien, la importancia de la pagina que vota determina lo importante que es el voto. Google calcula la importancia de una pagina a partir de los votos que obtiene. En el calculo del PageRank de una pagina se tiene en cuenta lo importante que es cada voto. HITS (Hyperlink.induced topic research) es un algoritmo que interactivo que tiene como finalidad excavar el grafo de la Web para identificar "hubs" y "authorities". Entendemos como authorities a las paginas que de acuerdo a un topico son las que mejor posicionadas estan. Los hubs son aquellas paginas que hacen liga hacia las authorities. El numero y el peso de hubs apuntando a una pagina determina el nivel de posicionamiento.

6. Web Usage Mining
Los logs que se generan constantemente en los servidores debido a los requerimientos de los usuarios, generan un gran volumen de datos provenientes de dichas acciones. Recientemente este gran volumen de informacion relevante empezo a usarse para obtener datos estadisticos, analizar accesos invalidos y para analizar problemas que se produjeran en el servidor. Los datos almacenados en los logs siguen un formato standard. Una entrada en el log siguiendo este formato contiene entre otras cosas, lo siguiente: direccion IP del cliente, identificacion del usuario, fecha y hora de acceso, requerimiento, URL de la pagina accedida, el protocolo utilizado para la transmision de los datos, un codigo de error, agente que realizo el requerimiento, y el numero de bytes transmitidos. Esto es almacenado en un archivo de texto separando cada campo por comas (",") y cada acceso es un renglon distinto Association Rules, Sequential Patterns y Clustering o Clasificacion son algunas de las tecnicas de data mining que se aplican en los servidores web.

6.1 Association Rules

Web Mining: Fundamentos Basicos

9

La Association Rules juega un papel muy importante en el contexto de la nueva vision de la web, es decir con el auge de las tecnicas de comercio que se manejan de forma electronica permiten el desarrollo de estrategias voraces de marketing. Normalmente esta tecnica esta relacionada con el uso de Bases de Datos transaccionales, donde cada transaccion consiste en un conjunto de items. En este modelo, el problema consiste en descubrir todas las asociaciones y correlaciones de items de datos donde la presencia de un conjunto de items en una transaccion implica la presencia de otros items. Esta tecnica generalmente esta asociada con en el numero de ocurrencias de los items dentro del log de transacciones[ 15], por lo tanto, podemos identificar la cantidad de usuarios que acceden a determinadas paginas (60% de los clientes que acceden a la pagina con URL /company/products/, tambien acceden a la pagina /company/products/product1.html). Por otro lado nos permite mejorar considerablemente la estructura de nuestro site, por ejemplo, si descubrimos que el 80% de los clientes que acceden a /company/products y /company/products/file1.html tambien acceden a /company/products/file2.html, parece indicar que alguna informacion de file1.html lleva a los clientes a acceder a file2.html. Esta correlacion podria sugerir que esta informacion deberia ser movida a /company/products para aumentar el acceso a file2.html.

6.2 Sequential Patterns En general en las Bases de Datos transaccionales se tienen disponibles los datos en un periodo de tiempo y se cuenta con la fecha en que se realizo la transaccion; la tecnica de sequential patterns se basa en descubrir patrones en los cuales la presencia de un conjunto de items es seguido por otro item en orden temporal. En el log de transacciones de los servidores de Web, se guarda la fecha y hora en la que un determinado usuario realizo los requerimientos. Analizando estos datos, se puede determinar el comportamiento de los usuarios con respecto al tiempo. Con esto, se puede determinar por ejemplo: 
10 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

En general todas las herramientas que realizan mining sobre el log enfocan el analisis sobre secuencias de tiempo ya que los eventos que son almacenados estan muy relacionados con el tiempo en que se producen.

6.3 Clustering Las tecnicas de clasificacion permiten desarrollar un perfil para los items pertenecientes a un grupo particular de acuerdo con sus atributos comunes. Este perfil luego puede ser utilizado para clasificar nuevos items que se agreguen en la base de datos. En el contexto de Web Mining, las tecnicas de clasificacion permiten desarrollar un perfil para clientes que acceden a paginas o archivos particulares, basado en informacion demografica disponible de los mismos. Esta informacion puede ser obtenida analizando los requerimientos de los clientes y la informacion transmitida de los browsers incluyendo el URL. Utilizando tecnicas de clasificacion, se puede obtener, por ejemplo, lo siguiente: 50% de los clientes que emiten una orden on-line en /company/products/product2.html, estan entre 20 y 25 anos y viven en la costa oeste. La informacion acerca de los clientes puede ser obtenida del browser del cliente automaticamente por el servidor; esto incluye los accesos historicos a paginas, el archivo de cookies, etc. Otra manera de obtener informacion es por medio de las registraciones y los formularios on-line. La agrupacion automatica de clientes o datos con caracteristicas similares sin tener una clasificacion predefinida es llamada clustering.

7. Conclusiones
La Web Mining ha despertado gran interes en la actualidad, particularmente debido a los avances de la comunidad cientifica en distintas lineas de investigacion relacionadas con Data Mining orientado a la www. La web mining en los ultimos anos esto se ha potenciado fuertemente en virtud del gran aumento en volumen del trafico, tamano y complejidad de las fuentes de informacion disponibles en la Web y el reciente interes en el desarrollo aplicaciones para el comercio electronico. La iniciativa privada actualmente es el principal precursor de que la informacion sea particular para cada individuo creando sistemas que incorporan personalizacion construyen modelos de los objetivos, caracteristicas, preferencias y conocimientos de cada usuario. La web mining es un area con expectro amplio de investigacion. En este documento se hace una simple aproximacion para tener ciertas bases en proyectos profundos de investigacion.

Web Mining: Fundamentos Basicos

11

8. Referencias

1.

Baeza-Yates, R. Castillo, C. Marin, M. and Rodriguez, A. "Crawling a Country: Better Strategies than BreadthFirst for Web Page Ordering", WWW Conference / Industrial Track, ACM, pp. 864-872. Chiba, Japan, 2005. Baeza-Yates, R. Excavando la web. El profesional de la informacion. v13, n1, 2004 Liu, B. and Chen-Chuan Chang, Kevin. Editorial: "Special Issue on Web Content Mining". WWW 2005 Tutorial, Page 1-4, 2005. Scotto, M. Sillitti, A. Succi, G. Vernazza, T. "Managing Web-Based Information", International Conference on Enterprise Information Systems (ICEIS 2004), Porto, Portugal, April 2004. Page 1-3 Etzioni O. "The World Wide Web: quagmire or gold mine?" , In Communications of the ACM 39(11). 1996 www.wikipedia.org Henzinger M. "Web Information Retrieval an Algorithmic Perspective", European Symposium on Algorithms, p 1-3, 2000 Baeza-Yates, R. Pobrete, B. "Una herramienta de mineria de consultas para el diseno del contenido y la estructura de un sitioWeb" Actas del III Taller Nacional de Mineria de Datos y Aprendizaje TAMIDA2005", pp.39-48, 2005 Kosala, R. and Blockeel, H. Web Mining Research: A Survey. ACM SIGKDD Explorations, Newsletter of the Special Interest Group on Knowledge Discovery and Data Mining. Page 1-9, 2000. Galeas, P. Web Mining by Patricio Galeas. http://www.galeas.de/webmining.html Wilks, Y. Information Extraction as a Core Language Technology Source Lecture Notes In Computer Science; Vol. 1299 Pages: 1 ­ 9, 1997

2. 3. 4.

5. 6. 7. 8.

9.

10. 11.

12 Francisco Manuel De Gyves Camacho Doctorado en informatica y automatica Universidad de Salamanca

12.

J. Wang, Y. Huang, G. Wu, and F. Zhang, Web Mining: Knowledge Discovery on the Web Proc. Int'l Conf. Systems, Man and Cybernetics (SMC '99), vol. 2, pp. 137-141 Wang, J. Huang, Y. Wu, G. and F. Zhang, "Web Mining: Knowledge Discovery on the Web" Proc. Int'l Conf. Systems, Man and Cybernetics (SMC '99), vol. 2, pp. 137-141, 1999. Eikvil, L. "Information Extraction from World Wide Web - A Survey", Rapport Nr. 945, July, 1999. ISBN 82-539-0429-0 http://www2.ing.puc.cl/gescopp/Sergio_Maturana/SAG/Webmining.html Wang, B. Liu, Z. "Web Mining Research," Fifth International Conference on Computational Intelligence and Multimedia Applications (ICCIMA'03), iccima, p. 84, 2003.

13.

14. 15. 16.

