\section{Decisiones de diseño} \label{decisiones}
En las siguientes subsecciones se van a detallar las decisiones de diseño que se han tenido en cuenta a la hora de desarrollar e implementar este sistema. Algunas de estas decisiones de diseño son las mismas que se tomaron cuando se desarrolló el módulo de indexación de documentos, aunque se han realizado algunos cambios. 

\subsection{Lenguaje de programación y sistema operativo elegido}

Para implementar el sistema, se ha decidido utilizar el lenguaje de
programación Python (\cite{Python}). Se ha seleccionado este lenguaje
de programación ya que permite el uso de estructuras como diccionarios
(tablas hash), listas, manejadores de archivos, llamadas al sistema,
definición de patrones mediante expresiones regulares, etc. Dichos
elementos han facilitado el desarrollo del sistema, ya
que Python los gestiona de una manera eficaz y permite su uso de uso
de una manera sencilla. 

Por otra parte, se ha elegido un sistema UNIX porque resulta más
cómodo a la hora de llevar a cabo la implementación. 

\subsection{Diseño del sistema}

\subsubsection{Diseño multicapa}
El sistema se ha desarrollado siguiendo el enfoque multicapa, para
desacoplar las operaciones del dominio de las operaciones de
persistencia y presentación, consiguiendo así un sistema extensible y
reutilizable. 

Uma muestra de reutilización es que las clases de la capa de presentación conocen a las
clases de dominio, pero no al revés, por
lo que podría cambiarse la interfaz de usuario sin tener que cambiar
las clases de dominio. 

Por otra parte, el dominio es totalmente independiente de la persistencia, pues se implementan el patrón DAO (también llamado Fabricación Pura), encargados de gestionar la persistencia, tanto en la base de datos como en un fichero. Además, el DAO que se usa para almacenar la información en la base de datos, utiliza a su vez un patrón Agente Singleton, encargado de inicializar la conexión de la base de datos si no existe ya una
instancia del agente, cerrar la conexión y ejecutar las sentencias SQL que recibe del patrón DAO. 

De este modo, el patrón DAO se ha utilizado para
desacoplar el dominio de la persistencia, evitando así que las propias
clases del dominio creen las secuencias SQL necesarias para gestionar
la base de datos, pasando ésto a ser responsabilidad del patrón
DAO. Por tanto, si en algún momento se cambia el sistema gestor de
base de datos, las clases de dominio no se verían afectadas, ya que
sólo habría que cambiar el agente y el DAO.

Esta división en capas y las diferentes clases diseñadas e implementadas se detallan en el apartado siguiente.

\subsubsection{Diagrama de clases}
En la Figura \ref{fig:clases} se muestran las clases que conforman el sistema. Como puede observarse en la figura, y como se mencionó en el apartado anterior, se han diseñado tres capas (representadas por paquetes) que permiten desacoplar unas clases de otras, ya que sólo se utilizan dependencias de uso entre clases.

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.3]{./images/SearchEngine}
	\caption{Diagrama de clases del sistema desarrollado}
	\label{fig:clases}
\end{figure} 

A continuación, se explica brevemente la responsabilidad de algunas de las clases más importantes.

\begin{milista}
	\item \textbf{Analyzer}: se encarga de leer el fichero o los ficheros del directorio indicado. Para cada fichero, llama al módulo \textit{parser}, hace una copia del documento en el repositorio del sistema e inserta su título y ruta en la base de datos.
        \item \textbf{Vector}: se utiliza para crear los vectores de cada documento indicando el título del documento, los términos del documento y sus pesos. La función \texttt{get$\_$similarity} calcula la semenjanza entre la pregunta y el vector documento (ver Ecuación \eqref{eq:formula}).
        \item \textbf{Utilities}: es una clase intermedia que se encuentra en la capa de dominio. Su función se limita a establecer una conexión entre las capas de presentación y persistencia.
        \item \textbf{BdDao}: construye las sentencias SQL y delega su ejecución en el Agente. El objetivo de esta clase es separar las capas de dominio y persistencia.
        \item \textbf{FileDao}: se encarga de construir la estructura del fichero XML que se genera después de cada búsqueda. Delega la escritura de éste a la clase \textit{fileHandler} que es la encargada de leer y escribir ficheros en disco.
        \item \textbf{exception}: es un módulo que contiene excepciones personalizadas.
        \item \textbf{Config}: contiene la definición de constantes que, de algún modo, configuran ciertos aspectos del sistema (p.e. rutas de directorios y tamaño de la caché).
\end{milista}

\subsection{Diseño de la base de datos} \label{basedatos}

Como sistema gestor de base de datos se ha optado por utilizar MySQL,
ya que es ligero, libre, gratuito (ya que no se destina a fines
comerciales) y compatible con otros sistemas, como Windows. 

Por otra parte, como ya se comentó en la documentación del módulo de indexación, el diseño de la base de datos era el que se muestra en el diagrama de la Figura \ref{fig:BD}. 

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.75]{./images/BD}
	\caption{Diagrama de la base de datos antigua}
	\label{fig:BD}
\end{figure} 

Sin embargo, ha sido necesario modificar ligeramente este diseño, para optimizar las búsquedas y consumir menos tiempo. El cambio que se ha introducido es simplemente un cambio de la clave primaria de la tabla \textit{posting$\_$file}, pasando a tener una clave autonumérica. Con esto (y optimizando las sentencias SQL, como se detallará posteriormente), se ha reducido tanto el tiempo de búsqueda, como el de indexación.

Así, el nuevo diagrama de tablas se muestra en la Figura \ref{fig:newDB}, mientras que las tablas que existen en el sistema son: 

\begin{milista}
	\item \textbf{dic}: implementa el diccionario de
          términos. Tiene los campos \textit{term} y
          \textit{num$\_$docs}, que representan cada uno de los
          términos encontrados, junto con el número de documentos
          donde aparece cada término. La clave primaria es el término.
	\item \textbf{doc}: implementa la tabla de documentos. Tiene
          los campos \textit{id$\_$doc}, \textit{title} y
          \textit{path} para almacenar un identificador único de
          documento, el título del documento y la ruta del sistema
          donde se almacena una copia de dicho documento.  
	\item \textbf{posting$\_$file}: implementa el
          posting$\_$file. Contiene los campos \textit{id}, \textit{term},
          \textit{id$\_$doc} y \textit{frequency} para representar la
          frecuencia con la que aparece un término en un
          documento. Por tanto, el campo \textit{term} hace referencia
          a términos del diccionario, y el campo \textit{id$\_$doc}
          hace referencia a los documentos de la tabla de documentos. 
\end{milista}

\begin{figure}[h]
	\centering
		\includegraphics[keepaspectratio, scale=0.75]{./images/newBD}
	\caption{Diagrama de la nueva base de datos}
	\label{fig:newDB}
\end{figure} 

\paragraph{Optimización de las sentencias SQL en el indexador de documentos} En la primera versión que se desarrolló del módulo encargado de indexar los documentos, los términos que se iban encontrando en un documento, se iban insertando uno a uno en la tabla \textit{posting$\_$file} y en el diccionario, además de ir actualizando la frecuencia de aparición del diccionario, con una sentencia \textbf{update}. Esto hacía que el acceso a la base de datos ocupase gran parte del tiempo, ya que se realizaban numerosas sentencias \textbf{insert} y \textbf{update}, para actualizar la tabla \textit{posting$\_$file} y la tabla \textit{dic} con los términos que se iban parseando.

Estudiando la sintaxis completa de la sentencia \textbf{insert}, se decidió hacer una inserción de múltiples filas en la tabla \textit{posting$\_$file}, donde cada fila corresponde a cada uno de los términos encontrados en un documento (junto con el identificador del documento y su frecuencia). Así, la sentencia \textbf{insert} que se utiliza es similar a la siguiente, que aparece en el manual de MySQL:

\makebox{\textbf{INSERT INTO tbl$\_$name [(col$\_$name,...)] VALUES (expr),(expr), ...}}

Del mismo modo, la sentencia \textbf{insert} permite hacer cambios en una columna cuando se intenta insertar una fila con clave primaria duplicada. Por tanto, esto puede utilizarse para realizar inserciones múltiples y, a la vez, actualizaciones en el diccionario, ya que si el término no se encontraba en el diccionario, se insertará, y si ya se encontraba, ocurrirá un fallo de clave duplicada (porque la clave primaria de la tabla \textit{dic} es el término) y se actualizará con la frecuencia correspondiente. La sentencia utilizada para insertar y actualizar términos en el diccionario es el siguiente:

\makebox{\textbf{INSERT INTO tbl$\_$name [(col$\_$name,...)] VALUES (expr),(expr), ...}} \\ 
\indent \makebox{\textbf{ON DUPLICATE KEY UPDATE num$\_$docs=num$\_$docs+VALUES(num$\_$docs)}}

Con estas cambios, el tiempo de indexación se ha reducido unos 22 minutos con respecto al módulo de indexación de la primera versión. En la sección \ref{estadisticas} se muestran algunas estadísticas de uso, tanto del módulo antiguo como del nuevo sistema.

\paragraph{Optimización de las sentencias SQL en la búsqueda de documentos} En un primer momento, para obtener todos los datos (frecuencias, identificador del documento, titulo, etc.) de los documentos que tenían términos en común con la pregunta, se pensó en hacer varias sentencias \textbf{select} por separado, pero esto consumía demasiado tiempo para una búsqueda.

Por tanto, se optó por hacer una única consulta componiendo las tres tablas y obteniendo así todos los datos necesarios para calcular los pesos y la relevancia. Esta consulta es la que se muestra a continuación: 

\makebox{\textbf{SELECT posting$\_$file.term,posting$\_$file.id$\_$doc,frequency,num$\_$docs,title}} \\
\indent \makebox{\textbf{FROM posting$\_$file JOIN dic JOIN doc}} \\
\indent \makebox{\textbf{ON dic.term=posting$\_$file.term AND posting$\_$file.id$\_$doc IN}} \\
\indent \makebox{\textbf{(SELECT DISTINCT p1.id$\_$doc FROM posting$\_$file p1 WHERE}} \\
\indent \makebox{\textbf{p1.term IN (\textit{lista de términos de la pregunta}))}}

Con esa sentencia, la consulta tardaba más de 40 segundos (para el término ''linux'') en procesarse, por lo que era necesario realizar unos cambios. Estudiando el manual de la sentencia \textbf{select} de MySQL, se observó que se puede elegir el índice (clave ajena) con el cuál combinar las tablas, lo que puede acelerar la búsqueda. Además, se puede indicar que esa sentencia tenga alta prioridad, ejecutando la sentencia en la base de datos antes que cualquier otra. Por otro lado, se optó también por crear una vista temporal con los resultados de la subconsulta anterior, para luego combinar con esa vista, en vez de tener una consulta con una subconsulta. Así, obtenemos estas sentencias:

\makebox{\textbf{CREATE VIEW view$\_$name AS SELECT DISTINCT p1.id$\_$doc}}\\
\indent \makebox{\textbf{FROM posting$\_$file p1 USE INDEX (term) WHERE}} \\
\indent \makebox{\textbf{p1.term IN (\textit{lista de términos de la pregunta}))}}

\makebox{\textbf{SELECT HIGH$\_$PRIORITY posting$\_$file.term, posting$\_$file.id$\_$doc, frequency,}}\\
\indent \makebox{\textbf{num$\_$docs, title}} \\
\indent \makebox{\textbf{FROM posting$\_$file USE INDEX (term,id$\_$doc) JOIN view$\_$name v JOIN dic}}\\
\indent \makebox{\textbf{JOIN doc }} \\
\indent \makebox{\textbf{ON posting$\_$file.id$\_$doc=v.id$\_$doc AND dic.term=posting$\_$file.term}}\\
\indent \makebox{\textbf{AND posting$\_$file.id$\_$doc=doc.id$\_$doc}}

\subsection{Desarrollo del sistema}
Para terminar, en esta subsección se detallan el módulo indexador de documentos y el de búsqueda, comentando las decisiones tomadas.

Cabe destacar que en la aplicación se utiliza el módulo \textit{psyco}, que es un módulo externo que realiza una
compilación \textit{just-in-time} y que incrementa la
velocidad de ejecución del código Python (ya que éste no se
interpretará). Gracias a este módulo el tiempo de ejecución de la
aplicación se puede reducir hasta la mitad.

\subsubsection{Motor de indexación}

\paragraph{Conexión con la base de datos} En una primera iteración del
desarrollo de este módulo, la conexión con la base de datos se
creaba cada vez que se quería acceder a ella, como, por ejemplo, al
actualizar la frecuencia de un término en el \textit{posting$\_$file},
cerrándose al terminar el acceso. Esto consumía mucho tiempo y el
acceso a disco era muy elevado, por lo que se ha optado por
inicializar la conexión con la base de datos al lanzar el módulo de indexación, y
cerrarla cuando termina. 

\paragraph{Copia de documentos} Cuando se analiza un documento para
realizar su indexación, en un primer momento se copiaba el documento
en la base de datos documental del sistema leyendo línea por línea. Este proceso
era algo lento, por lo que finalmente se ha utilizado una llamada al
sistema implementada en C, consiguiendo que la copia del documento
completo sea instantánea.  

\paragraph{Parser} Para realizar el tratamiento de los documentos y
prepararlos para su indexación, se ha implementado el módulo
\textit{parser}. Dicho módulo recibe una línea y devuelve una lista con todos los términos contenidos en esa línea. Para ello, realiza las siguientes acciones:  
\begin{milista}
	\item En primer lugar, se escapan los caracteres
          ''$\backslash$'' y '' ' '' de las palabras que los
          contengan, ya que si se intenta insertar un término con esos
          caracteres en la base de datos, la sentencia sql no se forma de manera correcta
          y se provoca una excepción al ejecutar esa sentencia en la
          base de datos.  
	\item Se definen separadores de palabras, que son tanto los
          signos de puntuación, como los espacios en blanco
          (incluyendo saltos de línea, tabuladores, etc). Se define
          también un patrón para detectar direcciones IP y números.
	\item Todos los símbolos separadores que estén contenidos en la línea (menos el punto y el guión)se reemplazan por un espacio
          en blanco y se divide la línea en palabras, separando por espacios, obteniendo así una lista de palabras, que recibe
          el siguiente tratamiento: 
	\begin{enumerate}
	\item Si una palabra es un espacio en blanco o se encuentra en la \textit{stop$\_$list}, se ignora.
	\item Si una palabra se corresponde con el patrón IP, se almacena esa palabra en la lista de palabras que va a devolver el parser. 
	\item Si una palabra se corresponde con el patrón de un número, también se almacena ese número.
	\item Se reemplazan las vocales acentuadas de las palabras por vocales sin acentuar.
	\item Se reemplazan los puntos contenidos en la palabra por espacios en blanco. Se hace ahora y no antes, para no perder los puntos que contiene una dirección IP.
	\item Si una palabra tiene un guión, esa palabra se almacena con el guión y por separado, siempre y cuando las palabras por separado no estén en la stop$\_$list. Por ejemplo, si aparece \textit{cd-rom}, se almacenarían las palabras ''cd-rom'', ''cd'' y ''rom''.
	\item Una palabra que no comience con un caracter alfanumérico, se ignora.
	\end{enumerate}
\end{milista}

Con este tratamiento, sólo se almacenan direcciones IP, números y palabras, separando en palabras cualquier ruta, dirección web o e-mail.

\paragraph{Codificación de documentos y de la base de datos} Tras
realizar múltiples pruebas con diferentes documentos, se observaron
fallos a la hora de recuperar y almacenar términos en la base de
datos. Estos fallos eran debidos a la codificación de los documentos,
que debe ser UTF-8 para el corrrecto funcionamiento en sistemas
UNIX. Por tanto, todos los documentos de la colección se han
estandarizado a la codificación UTF-8, al igual que las diferentes
tablas de la base de datos. 

Aunque la codificación de la base de datos sea UTF-8, se han realizado pruebas con documentos escritos en codificación \textit{latin-1}, y la indexación funciona correctamente, aunque es algo más lenta y a veces aparecen términos con caracteres extraños, debido a dicha codificación.

\paragraph{Posting$\_$file} Se ha optado por almacenar el
posting$\_$file de cada documento en memoria en forma de diccionario
(tabla hash), para conseguir que los accesos sean más rápidos. Se han
realizado pruebas y para un documento de unas 25.000 líneas, el tamaño
del posting$\_$file no excede de los 13MB en memoria. Por tanto, esta
opción es más eficiente en cuanto a tiempo, ya que sólo se vuelca el diccionario a la base de datos cuando se termina de procesar cada documento, en lugar de ir accediendo a la base de datos por cada uno de los términos que se deben almacenar.

Otra de las razones para hacer esto es para poder aprovechar la sentencia \textbf{insert} múltiple que se comentó en el apartado \ref{basedatos}.

\paragraph{Memoria caché} Para disminuir el número de inserciones/actualizaciones en el diccionario, se ha decidido implementar una caché (en forma de tabla hash), donde se van almacenando términos, junto con el número de documentos en los que aparece dicho término. Así, cuando la caché está llena, se vacía a la base de datos, con la sentencia \textit{insert} que se detalló en el apartado \ref{basedatos}. También se vacía antes de insertar los términos de un documento en la tabla \textit{posting$\_$file}, para mantener la integridad de clave ajena que dicha tabla tiene. 

\subsubsection{Motor de búsqueda}

\paragraph{Searcher} Este módulo es el encargado de realizar la búsqueda de los documentos relevantes a una pregunta dada, siguiendo el modelo vectorial. Se ha implementado este modelo ya que no está sujeto a las limitaciones del modelo booleano y da unos resultados similares al modelo probabilístico siendo más sencilla su implementación. En cuanto a la aproximación implementada, se ha elegido la que calcula la semejanza de dos vectores en función del ángulo que forman, ya que es más precisa que la que calcula la distancia entre dos vectores. Esta segunda aproximación es más ineficiente debido a que los usuarios suelen buscar entre 3 y 5 términos de media y los documentos con pocos términos serían muy próximos a la pregunta.

Por tanto, la fórmula que se sigue en este modelo para calcular la relevancia entre un documento y la pregunta es la mostrada en la ecuación \eqref{eq:formula}.

\begin{equation}
sem(p, d_{i}) = cos (\alpha) = \dfrac{\sum_{j=1}^n wp_{j} w_{ij}}{\sqrt{\sum_{j=1}^n wp_{j}^2} \sqrt{\sum_{j=1}^n w_{ij}^2}}
\label{eq:formula}
\end{equation}

El funcionamiento de este módulo es el siguiente:
\begin{enumerate}
	\item Se aplica el módulo \textit{parser} a la pregunta que el usuario introduce, obteniendo una lista de términos con el peso introducido (1, por defecto).
	\item Se calcula el módulo de la pregunta, a partir de los pesos de cada uno de sus términos.
	\item Se consultan los términos y frecuencias de los documentos que tienen algún término en común con la pregunta, utilizando la sentencia \textbf{select} que se comentó en el apartado \ref{basedatos}.
	\item Se recorren las filas que la consulta devuelve, creando los vectores de los documentos encontrados. Dicho vector contiene el término y su peso, que se calcula con la ecuación \eqref{eq:pesos}.
	\item Se recorren los vectores de los documentos, para calcular su semejanza con la pregunta.
	\item Finalmente, se devuelve una lista con la información de los documentos ordenados de mayor a menor semejanza, además de escribir la información en el fichero XML.
\end{enumerate}

De este modo, al mismo tiempo que se van recorriendo las filas que devuelve la sentencia \textbf{select}, se van calculando los pesos y creando los vectores de lod documentos, ahorrando bucles y tiempo.

\begin{equation}
w_{ij} = ft_{ij} f_{i}d_{j} = ft_{ij} log(d/fd_{j})
\label{eq:pesos}
\end{equation}
